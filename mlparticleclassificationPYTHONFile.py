# -*- coding: utf-8 -*-
"""MLParticleClassification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13hFiQmVKGWaGV98Ax5Oe-ZrZKOVjsXq1

#ML Telescope Particle Classification

This ML exercise is based on the YTBE video by Kylie Ying's Machine Learning for Everybody course
https://www.youtube.com/watch?v=i_LwzRVP7bg
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler

"""Data for this example was taken from here, its licensed under creative commons by 4.0: Bock, R. (2004). MAGIC Gamma Telescope [Dataset]. UCI Machine Learning Repository. https://doi.org/10.24432/C52C8B."""

labels = ["fLength","fWidth","fSize","fConc","fConc1","fAsym","fM3Long","fM3Trans","fAlpha","fDist","class"]
dataSet = pd.read_csv(filepath_or_buffer="/content/magic04.data",names=labels)
dataSet["class"] = (dataSet["class"] == "g").astype(int)
dataSet["fArea"] = (dataSet['fWidth']*dataSet['fLength'])
dataSet["fDist_Alpha"] = dataSet["fDist"] * dataSet["fAlpha"]
dataSet.head()

"""loads data into a var, remember to add labels, classifies based on class:

G = 1

H = 0

#Validation, Training, DataSets
"""

dataSet.replace([np.inf, -np.inf], np.nan, inplace=True)
dataSet.fillna(dataSet.median(), inplace=True) # Fill gaps with the median
features = dataSet.columns.drop('class')
for data in features:
    plt.figure(figsize=(8, 5))
    plt.hist(dataSet[dataSet['class']==1][data], color='blue', label='gamma', alpha=0.5, density=True, bins=40)
    plt.hist(dataSet[dataSet['class']==0][data], color='red', label='hadron', alpha=0.5, density=True, bins=40)

    plt.title(f"Feature Analysis: {data}")
    plt.xlabel(f"Value of {data}")
    plt.ylabel("Probability Density")
    plt.grid(axis='y', alpha=0.3)
    plt.legend()
    plt.show()

train, valid, test = np.split(dataSet.sample(frac=1),[int(0.6*len(dataSet)), int(0.8*len(dataSet))])

def scaleDataSet(dataframe, oversample=False):
    x = dataframe.drop(columns=['class']).values
    y = dataframe['class'].values

    scaler = StandardScaler()
    x = scaler.fit_transform(x)

    if oversample:
        ros = RandomOverSampler()
        x, y = ros.fit_resample(x, y)
    data = np.hstack([x, np.reshape(y, (-1, 1))])

    return data, x, y

train, xTrain, yTrain = scaleDataSet(train,oversample=True)
valid, xValid, yValid = scaleDataSet(valid,oversample=False)
test, xTest, yTest = scaleDataSet(test,oversample=False)

"""#K Nearest Neightbors:
Essentially, lets use a X and Y graph, what it does is, lets say we want to find whether X=10 Y=10 is a Gamma or a Hadron, so what do we do?

We find the nearest N number of neighbors, closest to the point we are investigating, and do the distance equation:


SQRT((x1-x2)^2+(y1-y2)^2))

Then we figure out which value is the value of the mayority of neighbors, so if 8 out of the 10 neighbors of (10,10) are hadrons, there is a high chance (10,10) is also a hadron, this ofc can be scaled up to N number of dimensions we need to process, in this case, it would be a 10 dimensional K Nearest Neighbor
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

knnModel = KNeighborsClassifier(n_neighbors=5)
knnModel.fit(xTrain,yTrain)

yPred = knnModel.predict(xTest)

print(classification_report(yTest,yPred))

"""#Naive Bayes
Used to guess the posibility of X given Y, so like calculate what is the chance of an 'P' person having covid, and getting a false positive or a false negative, after taking a test, based on historic data

Essentially it just multiplies all of the individual probabilities
"""

from sklearn.naive_bayes import GaussianNB

nbModel = GaussianNB().fit(xTrain,yTrain)
yPred = nbModel.predict(xTest)
print(classification_report(yTest,yPred))

"""#Logistic Regression
Draw a plot, put a line somewhere, that line divides the class types, that is the basis of classification, but that doesnt work for data which isnt clear cut, a 0,3 or a 0,1 would break that, so what do we do? we essentially bend the line into an S or a sigmoid, so it allows you to fine tune it with a bunch of parameters to get better results
"""

from sklearn.linear_model import LogisticRegression

logModel = LogisticRegression().fit(xTrain,yTrain)
yPred = logModel.predict(xTest)
print(classification_report(yTest,yPred))

"""#Support Vector Machine
Find a line somewhere in a graph, that best separates data, its what you usually do when you are explaining what a classification ML does, if it takes more axis now we have plane.
How close the line is to the closest data matters here, you want it on the dead center of the 2 groups

Not as good as dealing with outliers or weird data, even if they are the only outliers

It works with single dimension data tho, which is poggers

SVC = Support Vector Classifier
"""

from sklearn.svm import SVC

svcModel = SVC().fit(xTrain,yTrain);
yPred = svcModel.predict(xTest);
print(classification_report(yTest,yPred));

"""#NEURALS NETS
The cool ones, one input layers, N number of hidden layer neurons, and the output

You give inputs, the inputs are modified by the neuron,for as many neurons there are, and all the neurons are added on every neuron, then gets biased before getting output

It uses an activation function to get an altered state of the value, instead of a linear sum of all the inputs + neuron value.

Works by generation, you can train this one by giving it the Loss Value, the goal is to reduce it as much as you can.

You run it, you get the old values, and sum it by an alpha, or a learning rate/how quickly you take steps, and that gives the new values to modify on the neurons

"""

!pip install optuna
!pip install optuna plotly
import tensorflow as tf
import optuna

def trainModel(xTrain,yTrain,numNodes,dropoutProb,lr,batchSize,epochs):
  early_stop = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=30,
    restore_best_weights=True
  )
  reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001
  )
  neuralNetModel = tf.keras.Sequential([
      tf.keras.Input(shape=(xTrain.shape[1],)),

      tf.keras.layers.Dense(numNodes,activation='relu'),
      tf.keras.layers.Dropout(dropoutProb),


      tf.keras.layers.Dense(numNodes,activation='relu'),
      tf.keras.layers.Dropout(dropoutProb),

      tf.keras.layers.Dense(1,activation='sigmoid')
    ])
  neuralNetModel.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='binary_crossentropy',metrics=['accuracy'])
  history = neuralNetModel.fit(xTrain,yTrain,epochs=epochs, batch_size=batchSize,validation_split=0.2,verbose=2,callbacks=[early_stop,reduce_lr])
  return neuralNetModel, history

def evaluate_best_params(trial):
  num_nodes = trial.suggest_int('num_nodes', 64, 512, step=32)
  dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)
  lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)
  batch_size = trial.suggest_categorical('batch_size', [32,64, 128])
  epochs = 100

  use_dist_alpha = trial.suggest_categorical('use_dist_alpha',[True,False])
  use_area = trial.suggest_categorical('use_area', [True, False])



  model, history = trainModel(xTrain, yTrain, num_nodes, dropout_prob, lr, batch_size, epochs)
  val_loss, val_acc = model.evaluate(xValid, yValid, verbose=0)

  tf.keras.backend.clear_session()

  return val_loss

study = optuna.create_study(direction="minimize")
study.optimize(evaluate_best_params,n_trials=20)

print(f"Best parameters: {study.best_params}")
print(f"Best val_loss: {study.best_value}")
optuna.visualization.plot_parallel_coordinate(study)

optuna.visualization.plot_optimization_history(study)

optuna.visualization.plot_param_importances(study)

final_model, history = trainModel(
    xTrain, yTrain,
    numNodes=study.best_params['num_nodes'],
    dropoutProb=study.best_params['dropout_prob'],
    lr=study.best_params['lr'],
    batchSize=study.best_params['batch_size'],
    epochs=100
)

y_pred = (final_model.predict(xValid) > 0.6).astype(int)
print(classification_report(yValid, y_pred))